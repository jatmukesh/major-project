{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Prescription Medicine Recognition\n",
    "\n",
    "This notebook implements a TensorFlow-based pipeline for recognizing medicine names from handwritten prescriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 12:19:26.682955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Cell: Imports and Environment Setup\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import string\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa05324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory growth enabled for 1 GPU(s).\n"
     ]
    }
   ],
   "source": [
    "# ✅ Enable dynamic GPU memory growth (prevents OOM at startup)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"✅ Memory growth enabled for {len(gpus)} GPU(s).\")\n",
    "else:\n",
    "    print(\"⚙️ No GPU detected. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical GPUs: 1, Logical GPUs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761049170.648140     845 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Cell: GPU Configuration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"Physical GPUs: {len(gpus)}, Logical GPUs: {len(logical_gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU detected. Training will fall back to CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: /home/mukesh_jat/test/major-project/dataset\n",
      "Training labels: dataset/Training/training_labels.csv\n",
      "Validation labels: dataset/Validation/validation_labels.csv\n",
      "Testing labels: dataset/Testing/testing_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell: Configuration Parameters\n",
    "DATA_ROOT = pathlib.Path('dataset')  # Root directory containing Training/Validation/Testing\n",
    "TRAIN_DIR = DATA_ROOT / 'Training'\n",
    "VAL_DIR = DATA_ROOT / 'Validation'\n",
    "TEST_DIR = DATA_ROOT / 'Testing'\n",
    "\n",
    "TRAIN_IMAGES_DIR = TRAIN_DIR / 'training_words'\n",
    "VAL_IMAGES_DIR = VAL_DIR / 'validation_words'\n",
    "TEST_IMAGES_DIR = TEST_DIR / 'testing_words'\n",
    "\n",
    "TRAIN_LABELS_FILE = TRAIN_DIR / 'training_labels.csv'\n",
    "VAL_LABELS_FILE = VAL_DIR / 'validation_labels.csv'\n",
    "TEST_LABELS_FILE = TEST_DIR / 'testing_labels.csv'\n",
    "\n",
    "IMAGE_COLUMN = 'IMAGE'  # column containing the image filename\n",
    "LABEL_COLUMN = 'MEDICINE_NAME'  # supervised target column; adjust if you want GENERIC_NAME instead\n",
    "\n",
    "OUTPUT_DIR = pathlib.Path('artifacts')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 512\n",
    "BATCH_SIZE = 4\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "MAX_LABEL_LENGTH = 64\n",
    "\n",
    "print(f'Data root: {DATA_ROOT.resolve()}')\n",
    "print(f'Training labels: {TRAIN_LABELS_FILE}')\n",
    "print(f'Validation labels: {VAL_LABELS_FILE}')\n",
    "print(f'Testing labels: {TEST_LABELS_FILE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder image already exists at /home/mukesh_jat/test/major-project/placeholder.png\n"
     ]
    }
   ],
   "source": [
    "# Cell: Placeholder Asset Creation\n",
    "placeholder_path = pathlib.Path('placeholder.png')\n",
    "if not placeholder_path.exists():\n",
    "    placeholder_image = Image.new('L', (IMG_WIDTH, IMG_HEIGHT), color=255)\n",
    "    placeholder_image.save(placeholder_path)\n",
    "    print(f'Created placeholder image at {placeholder_path.resolve()}')\n",
    "else:\n",
    "    print(f'Placeholder image already exists at {placeholder_path.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocabulary size (including blank): 69\n",
      "🧩 CTC blank index reserved at: 68\n",
      "🔠 Example mapping (first 10 chars): [(' ', 0), ('(', 1), (')', 2), ('-', 3), ('.', 4), ('/', 5), ('0', 6), ('1', 7), ('2', 8), ('3', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Cell: Character Vocabulary\n",
    "# ---------------------------------------------\n",
    "# Build vocabulary from medical lexicon or dataset labels\n",
    "\n",
    "DEFAULT_CHARSET = string.ascii_lowercase + string.ascii_uppercase + string.digits + ' -./()'\n",
    "\n",
    "def build_vocabulary(labels: List[str], extra_tokens: str = '') -> Tuple[Dict[str, int], Dict[int, str], int, int]:\n",
    "    \"\"\"\n",
    "    Builds a character vocabulary for CTC-based models.\n",
    "\n",
    "    Returns:\n",
    "      char_to_num: dict mapping visible characters to [0..N-1]\n",
    "      num_to_char: inverse dict mapping indices to characters\n",
    "      BLANK_INDEX: reserved index for the CTC blank (N)\n",
    "      VOCAB_SIZE: N + 1 total (including the blank)\n",
    "    \"\"\"\n",
    "    # Combine dataset characters + default charset + extras\n",
    "    characters = sorted(set(''.join(labels)) | set(DEFAULT_CHARSET) | set(extra_tokens))\n",
    "\n",
    "    # Assign IDs 0..N-1 to all visible characters\n",
    "    char_to_num = {ch: i for i, ch in enumerate(characters)}\n",
    "    num_to_char = {i: ch for ch, i in char_to_num.items()}\n",
    "\n",
    "    # Reserve last index for CTC blank\n",
    "    BLANK_INDEX = len(characters)\n",
    "    VOCAB_SIZE = len(characters) + 1\n",
    "\n",
    "    return char_to_num, num_to_char, BLANK_INDEX, VOCAB_SIZE\n",
    "\n",
    "\n",
    "def load_labels(annotation_file: pathlib.Path) -> List[str]:\n",
    "    \"\"\"Loads the label column from CSV or returns placeholder labels.\"\"\"\n",
    "    if not annotation_file.exists():\n",
    "        print(f\"{annotation_file} not found; returning placeholder labels.\")\n",
    "        return ['Paracetamol', 'Ibuprofen']\n",
    "    df = pd.read_csv(annotation_file)\n",
    "    if LABEL_COLUMN not in df.columns:\n",
    "        raise ValueError(f\"Expected column '{LABEL_COLUMN}' in {annotation_file}, found {list(df.columns)}\")\n",
    "    return df[LABEL_COLUMN].astype(str).tolist()\n",
    "\n",
    "\n",
    "# Build vocabulary from training labels\n",
    "raw_labels = load_labels(TRAIN_LABELS_FILE)\n",
    "CHAR_TO_NUM, NUM_TO_CHAR, BLANK_INDEX, VOCAB_SIZE = build_vocabulary(raw_labels)\n",
    "\n",
    "print(f'✅ Vocabulary size (including blank): {VOCAB_SIZE}')\n",
    "print(f'🧩 CTC blank index reserved at: {BLANK_INDEX}')\n",
    "print(f'🔠 Example mapping (first 10 chars): {list(CHAR_TO_NUM.items())[:10]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f5cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Data Loading Utilities\n",
    "# -----------------------------------------------------\n",
    "def read_image(path: tf.Tensor) -> tf.Tensor:\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.io.decode_png(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_image(image: tf.Tensor) -> tf.Tensor:\n",
    "    # Ensure float dtype in [0,1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    # 🔄 Rotate portrait images to landscape if needed\n",
    "    shape = tf.shape(image)\n",
    "    height = tf.cast(shape[0], tf.int32)\n",
    "    width = tf.cast(shape[1], tf.int32)\n",
    "    image = tf.cond(height > width,\n",
    "                    lambda: tf.image.rot90(image, k=1),\n",
    "                    lambda: image)\n",
    "\n",
    "    # ✅ Force a fixed square size for all images\n",
    "    image = tf.image.resize(image, [512, 512])\n",
    "    image.set_shape([512, 512, 1])\n",
    "\n",
    "    # ✨ Optional normalization / enhancements\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    image = tf.image.adjust_brightness(image, 0.05)\n",
    "    image = tf.image.adjust_contrast(image, 1.5)\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def augment_image(image: tf.Tensor) -> tf.Tensor:\n",
    "    # Random augmentations to improve robustness\n",
    "    image = tf.image.random_brightness(image, 0.15)\n",
    "    image = tf.image.random_contrast(image, 0.75, 1.25)\n",
    "    image = tf.image.rot90(image, k=tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32))\n",
    "    return image\n",
    "\n",
    "\n",
    "def encode_label_py(label: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encode a text label into integer indices compatible with CTC loss.\n",
    "    - Visible chars -> [0..N-1]\n",
    "    - Blank index (VOCAB_SIZE - 1) used for padding\n",
    "    \"\"\"\n",
    "    # Encode each character to its numeric index\n",
    "    encoded = [CHAR_TO_NUM.get(ch, CHAR_TO_NUM.get(' ', 0)) for ch in label.strip()]\n",
    "    encoded = encoded[:MAX_LABEL_LENGTH]\n",
    "\n",
    "    # Pad remaining positions with BLANK_INDEX (not -1)\n",
    "    pad_len = MAX_LABEL_LENGTH - len(encoded)\n",
    "    if pad_len > 0:\n",
    "        encoded = encoded + [BLANK_INDEX] * pad_len\n",
    "\n",
    "    return np.array(encoded, dtype=np.int32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def prepare_example(path: tf.Tensor, label: tf.Tensor, training: bool = False):\n",
    "    \"\"\"\n",
    "    Prepares a (image, label) pair for training.\n",
    "    Applies preprocessing and augmentation, then encodes the label.\n",
    "    \"\"\"\n",
    "    image = read_image(path)\n",
    "    image = preprocess_image(image)\n",
    "    if training:\n",
    "        image = augment_image(image)\n",
    "    label_encoded = tf.numpy_function(\n",
    "        func=lambda l: encode_label_py(l.decode('utf-8')),\n",
    "        inp=[label],\n",
    "        Tout=tf.int32\n",
    "    )\n",
    "    label_encoded.set_shape([MAX_LABEL_LENGTH])\n",
    "    return image, label_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d4625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3fd8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Dataset Pipeline\n",
    "def create_dataset(annotation_file: pathlib.Path, images_dir: pathlib.Path, training: bool = False) -> tf.data.Dataset:\n",
    "    if not annotation_file.exists() or not images_dir.exists():\n",
    "        print(f\"Either {annotation_file} or {images_dir} is missing. Using placeholder samples.\")\n",
    "        dummy_paths = tf.constant([str(pathlib.Path('placeholder.png').resolve())] * len(raw_labels))\n",
    "        dummy_labels = tf.constant(raw_labels)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dummy_paths, dummy_labels))\n",
    "    else:\n",
    "        df = pd.read_csv(annotation_file)\n",
    "        if IMAGE_COLUMN not in df.columns:\n",
    "            raise ValueError(f\"Expected column '{IMAGE_COLUMN}' in {annotation_file}, found {list(df.columns)}\")\n",
    "        if LABEL_COLUMN not in df.columns:\n",
    "            raise ValueError(f\"Expected column '{LABEL_COLUMN}' in {annotation_file}, found {list(df.columns)}\")\n",
    "        paths = df[IMAGE_COLUMN].apply(lambda p: str((images_dir / p).resolve())).tolist()\n",
    "        labels = df[LABEL_COLUMN].astype(str).tolist()\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    dataset = dataset.map(lambda p, l: prepare_example(p, l, training), num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(TRAIN_LABELS_FILE, TRAIN_IMAGES_DIR, training=True)\n",
    "val_ds = create_dataset(VAL_LABELS_FILE, VAL_IMAGES_DIR, training=False)\n",
    "test_ds = create_dataset(TEST_LABELS_FILE, TEST_IMAGES_DIR, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd034214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"crnn_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"crnn_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15872</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,015,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">657,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">35,397</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ image_input (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ permute (\u001b[38;5;33mPermute\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m15872\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m1,015,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │       \u001b[38;5;34m657,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m69\u001b[0m)        │        \u001b[38;5;34m35,397\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,887,621</span> (22.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,887,621\u001b[0m (22.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,885,061</span> (22.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,885,061\u001b[0m (22.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> (10.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,560\u001b[0m (10.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✅ Fixed CRNN Model Architecture for 512×512 Images\n",
    "def build_crnn_model(img_width: int, img_height: int, vocab_size: int) -> keras.Model:\n",
    "    # Input shape updated for 512×512 images\n",
    "    input_img = layers.Input(shape=(img_height, img_width, 1), name='image_input')\n",
    "\n",
    "    # --- Convolutional feature extractor ---\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(input_img)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1))(x)\n",
    "\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1))(x)\n",
    "\n",
    "    x = layers.Conv2D(512, (2, 2), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # --- Reshape feature map for sequence modeling ---\n",
    "    x = layers.Permute((2, 1, 3))(x)                # (batch, width, height, channels)\n",
    "    x = layers.TimeDistributed(layers.Flatten())(x) # (batch, width, height*channels)\n",
    "\n",
    "    # --- Recurrent layers ---\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n",
    "\n",
    "    # --- Output layer ---\n",
    "    output = layers.Dense(vocab_size, activation='softmax', name='dense_output')(x)\n",
    "\n",
    "    model = keras.Model(inputs=input_img, outputs=output, name='crnn_model')\n",
    "    return model\n",
    "\n",
    "\n",
    "# ✅ Build and summarize the model\n",
    "IMG_HEIGHT = 512\n",
    "IMG_WIDTH = 512\n",
    "\n",
    "crnn_model = build_crnn_model(IMG_WIDTH, IMG_HEIGHT, VOCAB_SIZE)\n",
    "crnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e576925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ CTC Loss and Model Wrapper\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype='int64')\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype='int64')\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(tf.reduce_mean(loss))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def build_ctc_model(base_model: keras.Model) -> keras.Model:\n",
    "    labels = layers.Input(name='label', shape=(MAX_LABEL_LENGTH,), dtype='int32')\n",
    "    y_pred = base_model.output\n",
    "    output = CTCLayer(name='ctc_loss')(labels, y_pred)\n",
    "    ctc_model = keras.Model(inputs=[base_model.input, labels], outputs=output)\n",
    "    return ctc_model\n",
    "\n",
    "\n",
    "ctc_model = build_ctc_model(crnn_model)\n",
    "ctc_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f79ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow successfully executed a matmul on: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Shows which device is being used for ops\n",
    "with tf.device('/device:GPU:0'):\n",
    "    a = tf.random.uniform((1000, 1000))\n",
    "    b = tf.random.uniform((1000, 1000))\n",
    "    c = tf.matmul(a, b)\n",
    "print(\"✅ TensorFlow successfully executed a matmul on:\", c.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "862ee15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 53 46 65 50 53 42 65 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n",
      " 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n",
      " 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 12:19:35.544646: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for img, lbl in train_ds.take(1):\n",
    "    lbl_np = lbl.numpy()\n",
    "    print(lbl_np[0])  # check one label sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3eb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373eb1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom training step defined. Prefer using ctc_model.fit with tf.data pipelines for full training.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Training Loop\n",
    "EPOCHS = 20\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = crnn_model(images, training=True)\n",
    "        ctc_layer = CTCLayer()\n",
    "        _ = ctc_layer(labels, logits)\n",
    "        loss = tf.add_n(ctc_layer.losses) if ctc_layer.losses else 0.0\n",
    "    gradients = tape.gradient(loss, crnn_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, crnn_model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "print('Custom training step defined. Prefer using ctc_model.fit with tf.data pipelines for full training.')\n",
    "# # Example usage with Keras fit:\n",
    "# history = ctc_model.fit(\n",
    "#     train_ds.map(lambda img, lbl: ({'image_input': img, 'label': lbl}, lbl)),\n",
    "#     validation_data=val_ds.map(lambda img, lbl: ({'image_input': img, 'label': lbl}, lbl)),\n",
    "#     epochs=EPOCHS,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding utilities ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Decoding Utilities\n",
    "@tf.function\n",
    "def greedy_decode(pred):\n",
    "    return tf.math.argmax(pred, axis=-1, output_type=tf.int32)\n",
    "\n",
    "\n",
    "def decode_batch_predictions(pred):\n",
    "    results = []\n",
    "    for text in pred:\n",
    "        text = tf.gather(text, tf.where(tf.not_equal(text, 0)))\n",
    "        text = tf.squeeze(text, axis=-1)\n",
    "        chars = [NUM_TO_CHAR.get(int(char), '') for char in text.numpy()]\n",
    "        results.append(''.join(chars))\n",
    "    return results\n",
    "\n",
    "\n",
    "def recognize_medicines(model: keras.Model, dataset: tf.data.Dataset) -> List[List[str]]:\n",
    "    medicines = []\n",
    "    for batch_images, _ in dataset:\n",
    "        preds = model.predict(batch_images)\n",
    "        decoded = decode_batch_predictions(greedy_decode(preds))\n",
    "        medicines.append(decoded)\n",
    "    return medicines\n",
    "\n",
    "print('Decoding utilities ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call `run_inference_example(crnn_model, ['path/to/image.png'])` after training.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Inference Example\n",
    "def run_inference_example(model: keras.Model, sample_paths: List[str]):\n",
    "    for path in sample_paths:\n",
    "        path_obj = pathlib.Path(path)\n",
    "        if not path_obj.exists():\n",
    "            print(f'Sample {path} not found. Skipping.')\n",
    "            continue\n",
    "        image = tf.io.read_file(str(path_obj))\n",
    "        image = tf.io.decode_png(image, channels=1)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = preprocess_image(image)\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "        preds = model.predict(image)\n",
    "        decoded = decode_batch_predictions(greedy_decode(preds))\n",
    "        print(f'{path}: {decoded[0]}')\n",
    "\n",
    "print(\"Call `run_inference_example(crnn_model, ['path/to/image.png'])` after training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Replace placeholder dataset paths with actual annotated prescription data.\n",
    "2. Ensure labels are properly encoded and aligned with the vocabulary.\n",
    "3. Train the model using `ctc_model.fit`.\n",
    "4. Integrate a medicine lexicon for post-processing corrections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Paths\n",
    "\n",
    "The training pipeline expects the dataset to be organized with the following directories relative to the project root:\n",
    "\n",
    "- Training images: `dataset/Training/training_words`\n",
    "- Validation images: `dataset/Validation/validation_words`\n",
    "- Testing images: `dataset/Testing/testing_words`\n",
    "\n",
    "CSV annotation files with the medicine labels are expected at:\n",
    "\n",
    "- Training labels: `dataset/Training/training_labels.csv`\n",
    "- Validation labels: `dataset/Validation/validation_labels.csv`\n",
    "- Testing labels: `dataset/Testing/testing_labels.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Fine-tune the CRNN directly from this notebook once the dataset folders are populated at:\n",
    "\n",
    "- `dataset/Training/training_words` with labels in `dataset/Training/training_labels.csv`\n",
    "- `dataset/Validation/validation_words` with labels in `dataset/Validation/validation_labels.csv`\n",
    "- `dataset/Testing/testing_words` with labels in `dataset/Testing/testing_labels.csv`\n",
    "\n",
    "The code below prepares remapped datasets, configures checkpoints and TensorBoard logging under `artifacts/`, and runs `ctc_model.fit`. Adjust the hyperparameters as needed for your hardware and data volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting Training\n",
    "\n",
    "If `ctc_model.fit` fails or exits early, walk through these checks before rerunning training:\n",
    "\n",
    "1. **Verify dataset paths** – confirm the `dataset/Training`, `dataset/Validation`, and `dataset/Testing` folders exist and contain the expected `*_words` subdirectories and CSV files. The helper cell below will print counts if everything is wired correctly.\n",
    "2. **Inspect CSV columns** – the annotation files must include both the `IMAGE` filename column and the `MEDICINE_NAME` label column. Any missing or misspelled header will raise a ValueError when building the datasets.\n",
    "3. **Spot-check a batch** – TensorFlow will fall back to the placeholder samples if directories are empty or paths are wrong. Review the sanity-check output and confirm you see real image paths, not `placeholder.png`.\n",
    "4. **Watch system resources** – large images or high batch sizes may exhaust GPU/CPU RAM. Reduce `BATCH_SIZE`, close other GPU jobs, or run on a machine with more memory if you observe OOM errors.\n",
    "5. **Confirm TensorFlow availability** – ensure the environment has TensorFlow installed with GPU support if desired (e.g., `pip install tensorflow==2.12.*` or `tensorflow-gpu`). Restart the kernel after installation.\n",
    "6. **Resume from checkpoints** – if training stops mid-way, reload the best checkpoint by calling `crnn_model.load_weights('artifacts/checkpoints/<file>.keras')` before restarting to avoid losing progress.\n",
    "\n",
    "Once these checks pass, rerun the callback configuration and the training cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training split]\n",
      "  ✓ Images directory found (3120 files with supported extensions)\n",
      "  Sample image files: 1251.png, 2946.png, 2997.png\n",
      "  ✓ Labels CSV found with 3120 rows and columns: ['IMAGE', 'MEDICINE_NAME', 'GENERIC_NAME']\n",
      "  Preview:\n",
      "    - 0.png -> Aceta\n",
      "    - 1.png -> Aceta\n",
      "    - 2.png -> Aceta\n",
      "[Validation split]\n",
      "  ✓ Images directory found (780 files with supported extensions)\n",
      "  Sample image files: 187.png, 318.png, 421.png\n",
      "  ✓ Labels CSV found with 780 rows and columns: ['IMAGE', 'MEDICINE_NAME', 'GENERIC_NAME']\n",
      "  Preview:\n",
      "    - 0.png -> Aceta\n",
      "    - 1.png -> Aceta\n",
      "    - 2.png -> Aceta\n",
      "[Test split]\n",
      "  ✓ Images directory found (780 files with supported extensions)\n",
      "  Sample image files: 187.png, 318.png, 421.png\n",
      "  ✓ Labels CSV found with 780 rows and columns: ['IMAGE', 'MEDICINE_NAME', 'GENERIC_NAME']\n",
      "  Preview:\n",
      "    - 0.png -> Aceta\n",
      "    - 1.png -> Aceta\n",
      "    - 2.png -> Aceta\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: Dataset Sanity Checks\n",
    "SUPPORTED_IMAGE_EXTS = ('.png', '.jpg', '.jpeg', '.bmp')\n",
    "\n",
    "def inspect_split(images_dir, labels_file, split_name):\n",
    "    print(f'[{split_name}]')\n",
    "    if not images_dir.exists():\n",
    "        print(f'  ✗ Images directory missing: {images_dir}')\n",
    "    else:\n",
    "        image_files = [p for p in images_dir.iterdir() if p.suffix.lower() in SUPPORTED_IMAGE_EXTS]\n",
    "        sample_files = sorted([p.name for p in image_files[:3]])\n",
    "        print(f'  ✓ Images directory found ({len(image_files)} files with supported extensions)')\n",
    "        if sample_files:\n",
    "            print('  Sample image files:', ', '.join(sample_files))\n",
    "        else:\n",
    "            print('  ⚠️ No files detected with extensions', SUPPORTED_IMAGE_EXTS)\n",
    "    if not labels_file.exists():\n",
    "        print(f'  ✗ Labels CSV missing: {labels_file}')\n",
    "        return\n",
    "    df = pd.read_csv(labels_file)\n",
    "    print(f'  ✓ Labels CSV found with {len(df)} rows and columns: {list(df.columns)}')\n",
    "    missing_cols = [col for col in [IMAGE_COLUMN, LABEL_COLUMN] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f'  ✗ Missing expected columns: {missing_cols}')\n",
    "    else:\n",
    "        preview = df[[IMAGE_COLUMN, LABEL_COLUMN]].head(3)\n",
    "        print('  Preview:')\n",
    "        for _, row in preview.iterrows():\n",
    "            print(f\"    - {row[IMAGE_COLUMN]} -> {row[LABEL_COLUMN]}\")\n",
    "\n",
    "inspect_split(TRAIN_IMAGES_DIR, TRAIN_LABELS_FILE, 'Training split')\n",
    "inspect_split(VAL_IMAGES_DIR, VAL_LABELS_FILE, 'Validation split')\n",
    "inspect_split(TEST_IMAGES_DIR, TEST_LABELS_FILE, 'Test split')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints directory: /home/mukesh_jat/test/major-project/artifacts/checkpoints\n",
      "TensorBoard logs directory: /home/mukesh_jat/test/major-project/artifacts/logs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = OUTPUT_DIR / 'checkpoints'\n",
    "tensorboard_log_dir = OUTPUT_DIR / 'logs'\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "tensorboard_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_inputs = train_ds.map(lambda img, lbl: ({'image_input': img, 'label': lbl}, lbl))\n",
    "val_inputs = val_ds.map(lambda img, lbl: ({'image_input': img, 'label': lbl}, lbl))\n",
    "test_inputs = test_ds.map(lambda img, lbl: ({'image_input': img, 'label': lbl}, lbl))\n",
    "\n",
    "print(f'Checkpoints directory: {checkpoint_dir.resolve()}')\n",
    "print(f'TensorBoard logs directory: {tensorboard_log_dir.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 12:19:46.186935: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 228ms/step - loss: 23.9502 - val_loss: 19.4087 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 225ms/step - loss: 18.9197 - val_loss: 26.9234 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 227ms/step - loss: 18.2551 - val_loss: 17.9745 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 225ms/step - loss: 17.5975 - val_loss: 17.4898 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 225ms/step - loss: 17.2958 - val_loss: 18.5042 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 224ms/step - loss: 17.0955 - val_loss: 17.5350 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 223ms/step - loss: 16.9322 - val_loss: 17.8795 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 226ms/step - loss: 16.7939 - val_loss: 17.3878 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 225ms/step - loss: 16.7135 - val_loss: 17.1457 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 225ms/step - loss: 16.5442 - val_loss: 16.5400 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 225ms/step - loss: 16.4186 - val_loss: 19.0092 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 224ms/step - loss: 16.3253 - val_loss: 16.1372 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 223ms/step - loss: 16.2111 - val_loss: 16.1718 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 223ms/step - loss: 16.0998 - val_loss: 19.9619 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 224ms/step - loss: 15.9541 - val_loss: 16.0805 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 224ms/step - loss: 15.8107 - val_loss: 19.8239 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 226ms/step - loss: 15.5986 - val_loss: 15.6877 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 223ms/step - loss: 15.5088 - val_loss: 15.5526 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 224ms/step - loss: 15.3780 - val_loss: 15.4811 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m780/780\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 224ms/step - loss: 15.1359 - val_loss: 16.6805 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(checkpoint_dir / 'crnn_{epoch:02d}.keras'),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(log_dir=str(tensorboard_log_dir)),\n",
    "]\n",
    "\n",
    "history = ctc_model.fit(\n",
    "    train_inputs,\n",
    "    validation_data=val_inputs,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "247d9054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference model saved at: /home/mukesh_jat/test/major-project/artifacts/inference_model.keras\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 📦 SAVE FINAL INFERENCE MODEL (for testing on new images)\n",
    "# ==========================================================\n",
    "\n",
    "# Save the base CRNN model (not the CTC training wrapper)\n",
    "inference_model_path = OUTPUT_DIR / \"inference_model.keras\"\n",
    "crnn_model.save(inference_model_path)\n",
    "print(f\"✅ Inference model saved at: {inference_model_path.resolve()}\")\n",
    "\n",
    "# To load it later:\n",
    "# from tensorflow import keras\n",
    "# model = keras.models.load_model(\"artifacts/inference_model.keras\", compile=False)\n",
    "\n",
    "# Example test (after loading):\n",
    "# run_inference_example(model, [\"path/to/unknown_prescription.png\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ba8ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794ms/step\n",
      "/mnt/c/Users/mukes/Desktop/dataset/dataset/Testing/testing_words/1.png: Ronin\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 🧪 LOAD THE TRAINED MODEL AND TEST ON NEW PRESCRIPTION\n",
    "# ==========================================================\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1️⃣ Load the trained model (from artifacts folder)\n",
    "model = keras.models.load_model(\"artifacts/inference_model.keras\", compile=False)\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "# 2️⃣ Path to your uploaded image\n",
    "# If running locally, give full path to the image on your PC\n",
    "# For your uploaded image in this chat, use the downloaded path below\n",
    "sample_image_path = \"/mnt/c/Users/mukes/Desktop/dataset/dataset/Testing/testing_words/1.png\"\n",
    "\n",
    "# 3️⃣ Run inference using the helper function defined earlier\n",
    "run_inference_example(model, [sample_image_path])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
